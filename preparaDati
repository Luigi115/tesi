import os
import random
import re
from collections import Counter, OrderedDict
from dataclasses import dataclass
from time import monotonic
from typing import Dict, List, Optional, Union

import numpy as np
import torch
import torch.nn as nn
from scipy.spatial.distance import cosine
from torch.utils.data import DataLoader
from datasets import Dataset, DatasetDict
from tqdm import tqdm

# Parametri configurabili per il modello Word2Vec
@dataclass
class Word2VecParams:
    # Parametri per lo skipgram
    MIN_FREQ = 5  # Frequenza minima di occorrenza di una parola per essere inclusa
    SKIPGRAM_N_WORDS = 3  # Numero di parole a sinistra e a destra di una parola centrale
    T = 85  # Percentuale di parole rare da scartare (utilizzata per il campionamento negativo)
    NEG_SAMPLES = 50  # Numero di campioni negativi per ogni parola centrale
    NS_ARRAY_LEN = 5_000_000  # Dimensione massima dell'array per il campionamento negativo
    SPECIALS = "<unk>"  # Token per parole sconosciute
    TOKENIZER = 'basic_english'  # Tipo di tokenizzazione (in questo caso per l'inglese base)

    # Parametri della rete neurale
    BATCH_SIZE = 64  # Numero di campioni per ogni batch di addestramento
    EMBED_DIM = 50  # Dimensione degli embedding (rappresentazioni delle parole)
    EMBED_MAX_NORM = None  # Limite massimo per la norma degli embedding (None = nessun limite)
    N_EPOCHS = 10  # Numero di epoche (cicli di addestramento)
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # Usa GPU se disponibile, altrimenti CPU
    CRITERION = nn.BCEWithLogitsLoss()  # Funzione di perdita (loss function) per il modello

# Funzione per ottenere i dati dai dataframe di addestramento e test
def get_data(train_dataframe, test_dataframe, column):
    # Ottiene i dati da dataframe pandas
    train_iter = Dataset.from_pandas(train_dataframe[[column]].reset_index(drop=True))
    display('train dataset', train_iter)
    
    valid_iter = Dataset.from_pandas(test_dataframe[[column]].reset_index(drop=True))
    display('test dataset', valid_iter)
    
    return train_iter, valid_iter

# Classe per gestire il vocabolario
class Vocab:
    def __init__(self, list, specials):
        # Crea dizionari per mappare parole a indici e viceversa
        self.stoi = {v[0]:(k, v[1]) for k, v in enumerate(list)}  # String-to-index
        self.itos = {k:(v[0], v[1]) for k, v in enumerate(list)}  # Index-to-string
        self._specials = specials[0]  # Token speciale per parole sconosciute
        self.total_tokens = np.nansum([f for _, (_, f) in self.stoi.items()], dtype=int)  # Numero totale di token

    def __len__(self):
        return len(self.stoi) - 1  # Restituisce la lunghezza del vocabolario (senza il token speciale)

    def get_index(self, word: Union[str, List]):
        # Restituisce l'indice di una parola o di una lista di parole
        if isinstance(word, str):
            if word in self.stoi:
                return self.stoi.get(word)[0]
            else:
                return self.stoi.get(self._specials)[0]
        elif isinstance(word, list):
            res = []
            for w in word:
                if w in self.stoi:
                    res.append(self.stoi.get(w)[0])
                else:
                    res.append(self.stoi.get(self._specials)[0])
            return res
        else:
            raise ValueError(f"Word {word} is not a string or a list of strings.")

    def get_freq(self, word: Union[str, List]):
        # Restituisce la frequenza di una parola o di una lista di parole
        if isinstance(word, str):
            if word in self.stoi:
                return self.stoi.get(word)[1]
            else:
                return self.stoi.get(self._specials)[1]
        elif isinstance(word, list):
            res = []
            for w in word:
                if w in self.stoi:
                    res.append(self.stoi.get(w)[1])
                else:
                    res.append(self.stoi.get(self._specials)[1])
            return res
        else:
            raise ValueError(f"Word {word} is not a string or a list of strings.")

    def lookup_token(self, token: Union[int, List]):
        # Restituisce la parola corrispondente a un indice (o una lista di indici)
        if isinstance(token, (int, np.int64)):
            if token in self.itos:
                return self.itos.get(token)[0]
            else:
                raise ValueError(f"Token {token} not in vocabulary")
        elif isinstance(token, list):
            res = []
            for t in token:
                if t in self.itos:
                    res.append(self.itos.get(token)[0])
                else:
                    raise ValueError(f"Token {t} is not a valid index.")
            return res

# Funzione per tokenizzare il testo
def yield_tokens(iterator, tokenizer, column):
    r = re.compile('[a-z1-9]')  # Regola per accettare solo caratteri alfanumerici
    for text in iterator:
        res = tokenizer(text[column])  # Tokenizza il testo
        res = list(filter(r.match, res))  # Filtra solo i token validi
        yield res

# Funzione per costruire il vocabolario
def vocab(ordered_dict: Dict, min_freq: int = 1, specials: str = '<unk>'):
    tokens = []
    # Aggiunge solo i token con frequenza maggiore o uguale a min_freq
    for token, freq in ordered_dict.items():
        if freq >= min_freq:
            tokens.append((token, freq))

    specials = (specials, np.nan)  # Aggiunge il token speciale
    tokens[0] = specials

    return Vocab(tokens, specials)

# Funzione pipeline per creare un vocabolario a partire da un testo
def pipeline(word, vocab, tokenizer):
    return vocab(tokenizer(word))

# Funzione per costruire il vocabolario dal dataset
def build_vocab(
        iterator,
        tokenizer,
        params: Word2VecParams,
        max_tokens: Optional[int] = None,
        column='text'
    ):
    counter = Counter()  # Conta la frequenza delle parole
    for tokens in yield_tokens(iterator, tokenizer, column):
        counter.update(tokens)

    # Ordina prima per frequenza decrescente, poi per ordine alfabetico
    sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: (-x[1], x[0]))

    ordered_dict = OrderedDict(sorted_by_freq_tuples)

    # Crea il vocabolario
    word_vocab = vocab(ordered_dict, min_freq=params.MIN_FREQ, specials=params.SPECIALS)
    return word_vocab

# Classe per generare coppie di parole per lo Skip-gram
class SkipGrams:
    def __init__(self, vocab: Vocab, params: Word2VecParams, tokenizer):
        self.vocab = vocab
        self.params = params
        self.t = self._t()  # Calcola il valore T per il campionamento negativo
        self.tokenizer = tokenizer
        self.discard_probs = self._create_discard_dict()  # Crea il dizionario per il campionamento negativo

    def _t(self):
        # Calcola la percentuale di parole rare da scartare
        freq_list = []
        for _, (_, freq) in list(self.vocab.stoi.items())[1:]:
            freq_list.append(freq/self.vocab.total_tokens)
        return np.percentile(freq_list, self.params.T)

    def _create_discard_dict(self):
        # Crea un dizionario che contiene la probabilit√† di scartare una parola
        discard_dict = {}
        for _, (word, freq) in self.vocab.stoi.items():
            discard_prob = 1 - np.sqrt(self.t / (freq/self.vocab.total_tokens + self.t))
            discard_dict[word] = discard_prob
        return discard_dict

    def collate_skipgram(self, batch):
        # Crea il batch di input e output per il modello skip-gram
        column = 'text'

        batch_input, batch_output  = [], []
        for text in batch:
            text = text[column]
            text_tokens = self.vocab.get_index(self.tokenizer(text))

            if len(text_tokens) < self.params.SKIPGRAM_N_WORDS * 2 + 1:
                continue  # Ignora sequenze troppo corte

            for idx in range(len(text_tokens) - self.params.SKIPGRAM_N_WORDS*2):
                token_id_sequence = text_tokens[idx : (idx + self.params.SKIPGRAM_N_WORDS * 2 + 1)]
                input_ = token_id_sequence.pop(self.params.SKIPGRAM_N_WORDS)  # Parola centrale
                outputs = token_id_sequence  # Parole di contesto

                # Campionamento negativo
                prb = random.random()
                del_pair = self.discard_probs.get(input_)
                if input_ == 0 or del_pair >= prb:
                    continue
                else:
                    for output in outputs:
                        prb = random.random()
                        del_pair = self.discard_probs.get(output)
                        if output == 0 or del_pair >= prb:
                            continue
                        else:
                            batch_input.append(input_)
                            batch_output.append(output)

        # Restituisce i batch come tensori per PyTorch
        batch_input = torch.tensor(batch_input, dtype=torch.long)
        batch_output = torch.tensor(batch_output, dtype=torch.long)

        return batch_input, batch_output
